title: Data Processing, Pattern computation
data: 2016-04-04
category: notes
tags: data processing, k-mer, notes

Based on the kernel methods, the problem of generic determinants selection can be formulated as following:
$$\min_{\beta\in\mathbb R^{p}}\frac{1}{n}\sum_{i=1}^{n}L\left(Y_i, \phi_i \cdot \beta\right)+\lambda \Omega(\beta)$$
where $L$ is a loss function and $\Omega$ is a regularization function.
The vector $\phi_i$ is a feature vector obtained by mapping the sequence $i$ to the Hilbert space $\mathbb R^p$.

There are different choices of the mapping function:

* spectrum kernel: $\phi_i$ denote the vector of the number of occurrences or the number of absences of each present k-mer.
* substring kernel

In all cases, the kernel function is supposed to be a comparison function or a similarity measure, and $\phi$ is a mapping projecting original space to a more easily comparable space.

Our first approach is to apply spectrum kernel for k-mers of different lengths. To have a straightforward feeling of the presence of k-mers of different lengths, we can also display some visualization using these feature matrices.

## Computing feature matrices
To compute the feature matrices in a reasonable time, I used a toolbox implemented in Python interface, called [SHOGUN][1]. This package allows to construct a dictionary representation of the k-mer vocabulary of the training set, in such a way that it maps each sequence to indices. After computing the vector of indices for each sequence, we can easily construct a sparse matrix $\phi$ in Python.
I have computed feature matrices for k-mers of length from 5 to 15, the space memory of these matrices becomes larger and larger but with lower growth for bigger k. And I have removed the k-mers that are presents 0 times in the sequences.

## Datasets presentation
The data is composed of 70975 sequences of DNA and the label vector $Y$ represents the strength of replication. The number of different k-mers of length $k$ is $4^k$. For example, if $k=10$, then the possibly maximal size of matrix $\phi$ is $70975\times 1048576$. The detailed information of matrices computed with k-mer of different lengths is shown in the table below.

| length | $p$   | size (GB) in npz format | maximal frequency | mean of frequencies |
| ------ | :---: |:-----------------------:|:-----:|:------:|
| 5      | 1024  |  0,053                  |70231  |57495.85|
| 6      | 4096  |  0.20                   |60379  |33127   |
| 7      | 16384 |  0.42                   |41340  |12926   |
| 8      | 65536 |  0.47                   |28902  |3950    |
| 9      | 262144 |  0.52                  |23242  |1069    |
| 10     | 1048576 |  0.66                 |19286  |275     |
| 11     | 4170068 |  0.83                 |16189  |70      |
| 12     | 15636635 |  0.94                |14104  |19      |
| 13     | 49091987 |  1.02                |12404  |6       |
| 14     | 115675030 |  1.07               |11698  |2.5     |
| 15     | 190409480 |  1.1                |9664   |1.54    |


## Analysis and visualization of feature matrices
One can compute easily the number of presences of each k-mer in sequences by replacing the values of matrix $\phi$ by 1 and summing along the columns.
```
k = 5 # length of k-mers
M = load_coo_matrix('data/features_matrix_'+str(k)+'.npz') # load matrix $\phi$
M.data[:] = 1 # without taking account of repetitions
S = M.sum(axis=0) # number of presences of each k-mer
```
##### Curves of minimal number of presences
![k-mer-presence]({filename}/images/kmer-presences/5.png)
![k-mer-presence]({filename}/images/kmer-presences/6.png)
![k-mer-presence]({filename}/images/kmer-presences/7.png)
![k-mer-presence]({filename}/images/kmer-presences/8.png)
![k-mer-presence]({filename}/images/kmer-presences/9.png)
![k-mer-presence]({filename}/images/kmer-presences/10.png)
![k-mer-presence]({filename}/images/kmer-presences/11.png)
![k-mer-presence]({filename}/images/kmer-presences/12.png)
![k-mer-presence]({filename}/images/kmer-presences/13.png)
![k-mer-presence]({filename}/images/kmer-presences/14.png)
![k-mer-presence]({filename}/images/kmer-presences/15.png)

With the growth of k, we can see that the number of presences of each feature gets lower and lower and eventually gets concentrated near 0. We can also set log-scale for axis $y$ for large k to visualize better the behavior near 0.

![k-mer-presence]({filename}/images/kmer-presences/log-13-2.png)
![k-mer-presence]({filename}/images/kmer-presences/log-14-2.png)
![k-mer-presence]({filename}/images/kmer-presences/log-15-2.png)

These figures give us an overview of presence of features and also an intuition to filter some k-mers which don't seem as important as others. For example, in the case k=15, the k-mers that only present once in the sequences occupy $\frac{3}{4}$ of total features. By consequence, if we remove all these features, the new feature matrix becomes 4 times smaller than the original one.

#### Distribution of presence for k-mers
![k-mer-presence]({filename}/images/kmer-presences/hist-5.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-6.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-7.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-8.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-9.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-10.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-11.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-12.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-13.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-14.png)
![k-mer-presence]({filename}/images/kmer-presences/hist-15.png)

[1]: http://www.shogun-toolbox.org/
