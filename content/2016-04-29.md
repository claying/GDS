title: More Experiments and Matrix compression
date: 2016-04-29
category: notes
tags: lasso regression, notes, intercept, compression

## More experiments on Lasso regression

I have carried out some more experiments on Lasso regression with the following transformations:

1. for the tf-idf transformation, I found a new commonly used transformation: log-entropy, which is defined as $\log(\text{tf}_{t,d}+1)\cdot (1+\sum_{d}\frac{p_{t,d}\log(p_{t,d})}{\log(n)})$. Here $p_{t,d}=\frac{\text{tf}_{t,d}}{\text{gf}_{t}}$ represents the probability that term $t$ appears in document $d$.
2. normalization by feature, i.e. normalization of each column of matrix $\phi$.
3. adding an intercept in the linear model, by, for example, adding a column with full one in matrix $\phi$.

The results showed that the two first transformations reduce the MSE in the cross validation while the third one didn't appear to improve the prediction.

![lasso]({filename}/images/lasso/intercept/logY_concatenate_filter1000/lasso_5.png)
![lasso]({filename}/images/lasso/intercept/logY_concatenate_filter1000/lasso_6.png)
![lasso]({filename}/images/lasso/intercept/logY_concatenate_filter1000/lasso_7.png)
![lasso]({filename}/images/lasso/intercept/logY_concatenate_filter1000/lasso_8.png)
![lasso]({filename}/images/lasso/intercept/logY_concatenate_filter1000/lasso_9.png)
![lasso]({filename}/images/lasso/intercept/logY_concatenate_filter1000/lasso_10.png)

After all these experiments, we have the following tracks to work on:

* How to resume the nature of the selected features or patterns? Can we visualize some clusters of the similar features by using a kernel PCA?
* Is it possible to improve the parsimony by using some more aggressive Lasso such as "weighted norm l1"?
* Is it possible to accelerate the optimization by e.g. applying some hashing tricks?

To answer the third question, I have tried some classic matrix decomposition methods.

## Matrix compression
Problem: Given an m-by-n matrix $\phi$, how to find an approximation of $\phi$ in the form of $UV$ with $U$ m-by-k matrix and $V$ k-by-n matrix?

This problem is quite connected with low-rank factorization problem or column subset selection problem. To resolve this kind of problems, there are different approaches:

* QR factorization (or RRQR factorization): $\phi=QR$ with $Q$ orthogonal and $R$ upper triangular.
* SVD factorization: $\phi=UDV^T$ with $U$, $V$ unitary and $D$ nonnegative diagonal matrix.
* CUR decomposition: $\phi=CUR$, where $C$ m-by-k matrix is made from the columns of $\phi$, $R$ k-by-n is made from the rows of $\phi$ and $U$ k-by-k matrix.

In order to conserve the sparsity of the matrix $\phi$, the SVD factorization is not appropriate here. However, the CUR decomposition or the column subset selection is suitable here because they select exactly a subset of columns. The principal sampling techniques are listed as follows:

* norm sampling
* leverage score sampling
* iterative norm sampling
