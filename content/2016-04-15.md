title: Ridge Regression (II)
data: 2016-04-07
category: notes
tags: ridge regression, notes, compression

Last time, I compared the prediction results for different transformations on the matrix $\phi$. However, I didn't get any conclusion because the performance of the transformations varies as k increases. Hence, to get a more convincing comparison among these transformations, I have concatenated the feature matrices associated with k-mers of length smaller than k, i.e. $\phi$ corresponds to the matrix containing the frequencies of all the k-mers of length smaller than k. Under this configuration, we could also understand how adding longer k-mers would impact the prediction accuracy.

The difficulty is when k increases, the size of the feature matrix will become immense so that it cannot be easily stocked in the memory. In this case, we need to compress the matrix by , for example, eliminating the features which have the save binary frequency. So our next step is to compress the feature matrix in order to make the computing more feasible and faster for large k.

## Concatenation and results

I took the same configuration as I had done last time except that I concatenated the matrices $\phi$.

![ridge]({filename}/images/ridge/logY_concatenate/ridge_5.png)
![ridge]({filename}/images/ridge/logY_concatenate/ridge_6.png)
![ridge]({filename}/images/ridge/logY_concatenate/ridge_7.png)
![ridge]({filename}/images/ridge/logY_concatenate/ridge_8.png)

One can remark that as k increases, the MSE doesn't visually decrease. This is due to the fact that when p increases, the regularization becomes more difficult.

## Compression of feature matrices

There are various approaches to compress matrix in order to reduce the size of the matrix $\phi$ as well as to remove some apparently redundant features. I listed below some ideas:

* eliminate features with the same binary frequency
* eliminate features with binary frequency smaller than a threshold (how to choose this threshold ?)
* group the features using hashing trick (e.g. [locality-sensitive hashing][1])
* eliminate less informative samples
* explore special structure inside the sequences (e.g. [De Bruijn graph][2])

In the following, I put the curves of cross validation for $\phi$ after compression from k=7 since for smaller k, the compression is not efficient enough. Concerned with the compression, I have applied only the two first filtering with threshold=5000. I observed that the MSE decreased after the compression probably due to the reason mentioned above. And the training error didn't decrease to 0, maybe the filtering is too brutal.

![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_7.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_8.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_9.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_10.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_11.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_12.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_13.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_14.png)






[1]: https://en.wikipedia.org/wiki/Locality-sensitive_hashing
[2]: https://en.wikipedia.org/wiki/De_Bruijn_graph
