title: Ridge Regression (II)
data: 2016-04-15
category: notes
tags: ridge regression, notes, compression, concatenation

Last time, I compared the prediction results for different transformations on the matrix $\phi$. However, I didn't get any conclusion because the performance of the transformations varies as k increases. Hence, to get a more convincing comparison among these transformations, I have concatenated the feature matrices associated with k-mers of length smaller than k, i.e. $\phi$ corresponds to the matrix containing the frequencies of all the k-mers of length smaller than k. Under this configuration, we could also understand how adding longer k-mers would impact the prediction accuracy.

The difficulty is when k increases, the size of the feature matrix will become immense so that it cannot be easily stocked in the memory. In this case, we need to compress the matrix by , for example, eliminating the features which have the save binary frequency. So our next step is to compress the feature matrix in order to make the computing more feasible and faster for large k.

## Concatenation and results

I took the same configuration as I had done last time except that I concatenated the matrices $\phi$.

![ridge]({filename}/images/ridge/logY_concatenate/ridge_5.png)
![ridge]({filename}/images/ridge/logY_concatenate/ridge_6.png)
![ridge]({filename}/images/ridge/logY_concatenate/ridge_7.png)
![ridge]({filename}/images/ridge/logY_concatenate/ridge_8.png)

One can remark that as k increases, the MSE doesn't visually decrease. This is due to the fact that when p increases, the regularization becomes more difficult.

## Compression of feature matrices

There are various approaches to compress matrix in order to reduce the size of the matrix $\phi$ as well as to remove some apparently redundant features. I listed below some ideas:

1. eliminate features with the same binary frequency
2. eliminate features with binary frequency smaller than a threshold (how to choose this threshold ?)
3. group the features using hashing trick (e.g. [locality-sensitive hashing][1])
4. eliminate less informative samples
5. explore special structure inside the sequences (e.g. [De Bruijn graph][2])

| length ($\le k$) | p | compression 1 (% of compression) | compression 2 with threshold=1000 (% of compression)| compression 2 with threshold=5000 (% of compression)|
| ---------------- | -- | ----------|-----------| ---------|
| 5 | 1024 | 1024 (0%) | 1024 (0%) | 1024 (0%) |
| 6 | 5120 | 5120 (0%) | 5120 (0%) | 5100 (0.39%)|
| 7 | 21504 | 21504 (0%) | 21416 (0.41%) | 18003 (16.3%) |
| 8 | 87040 | 87040 (0%) | 74248 (14.7%) | 37147 (57.3%) |
| 9 | 349184 | 349184 (0%) | 177357 (49%) | 40652 (88%) |
| 10 | 1397700 | 1397693 (0.005%) | 212164 (85%) | 41155 (97%) |
| 11 | 5567768 | 5541078 (0.5%) | 217449 (96%) | 41466 (99.3%) |
| 12 | 21204403 | 19719524 (7%) | 220033 (99.0%) | 41722 (99.8%) |
| 13 | 70296390 | 55403413 (21%) | 222172 (99.7%) | 41938 (99.9%) |
| 14 | 185971420 | | 224019 (99.9%) | 42108 (99.98%) |

--------
In the following, I put the curves of cross validation for $\phi$ after compression from k=7 since for smaller k, the compression is not efficient enough. Concerned with the compression, I have applied only the two first filtrations with threshold=5000. I observed that the MSE decreased after the compression probably due to the reason mentioned above. And the training error didn't decrease to 0, maybe the filtration is too brutal. The error curves become almost unchanged from k=8, which also implies that the filtration is too brutal.

Specifically, the transformation of $\phi$ can be resumed as follows:
```
threshold -> concatenation -> compression -> tf-idf -> normalization
```

![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_7.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_8.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_9.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_10.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_11.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_12.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_13.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter5000/ridge_14.png)

-------
I have changed the threshold from 5000 to 1000 and added another manner of coding: presence/absence or binary frequency. This way of coding shows a smaller MSE than above. I also observed that the MSE for the same transformation in this case (threshold=1000) is bigger than that of the case above (threshold=5000), the reason is perhaps more features are more difficult to regularize. From k=9, the running time of the program becomes very large, we need to do some intelligent hashing on the feature matrices to reduce it. Thus, I display the running time figure in terms of p.

![ridge_time]({filename}/images/time/ridge.png)

![ridge]({filename}/images/ridge/logY_concatenate_filter1000/ridge_5.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter1000/ridge_6.png)
![ridge]({filename}/images/ridge/logY_concatenate_filter1000/ridge_7.png)


[1]: https://en.wikipedia.org/wiki/Locality-sensitive_hashing
[2]: https://en.wikipedia.org/wiki/De_Bruijn_graph
