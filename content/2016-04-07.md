title: Preprocessing, Ridge Regression
data: 2016-04-07
category: notes
tags: ridge regression, preprocessing, notes

In a first stage, I applied the ridge regression to the problem in order to get a benchmark. More precisely, the original problem can be reformulated:

$$\min_{\beta\in\mathbb R^{p}}\frac{1}{2}\left\|Y- \phi \cdot \beta\right\|_{2}^2+\frac{\lambda}{2} \|\beta\|_2^2$$

where the loss function has been replaced by the square loss and the regularization function has been replaced by the $\ell_2$ norm.

This problem has already been extensively studied and there are various approaches to resolve it, such as the reformulation with kernel matrix, proximal method with gradient descent. Before applying these optimization methods, we need to preprocess the data in order to reduce the predictor sensibilities and the impact of data skewness or outliers.

## Preprocessing
Transformations of training set data may significantly improve the performance of the prediction. And there different ways and strategies to transform $Y$ and $\phi$.

According to the book **Applied Predictive Modeling** (Max Kuhn, Kjell Johnson 2013), The principal transformation methods are centering, scaling and skewness resolution.

##### Outcome variable $Y$
For the vector $Y$, since it is the outcome variable representing the strength of replication, we need the transformation to be reversible or at least monotone. By consequence, we consider especially the transformations resolving skewness. The formulae for the sample skewness statistic is defined:

$$skewness=\frac{\sum (Y_i-\overline Y)^3}{(n-1)v^{3/2}}$$

where $v=\frac{\sum (Y_i-\overline Y)^2}{n-1}$. An un-skewed distribution is one that is roughly symmetric. To reduce the skewness, we can apply a class of transformations proposed by Box and Cox with a parameter $\lambda$:

$$Y^*=
\begin{cases}
\frac{Y^{\lambda}-1}{\lambda} & \text{if } \lambda\neq 0 \\
\log(Y) & \text{if } \lambda = 0
\end{cases}
$$

I display in the following the distribution of $Y$ as well as its distribution after Box-Cox transformation.

| $\lambda$ | skewness |
| --------- | -------- |
| original  | 8.28     |
| 0         | 0.57     |
| -0.25     | 0.015    |

![distribution-of-Y]({filename}/images/preprocessing/hist_y.png)
![distribution-of-Y]({filename}/images/preprocessing/hist_log_y.png)
![distribution-of-Y]({filename}/images/preprocessing/hist_box_cox.png)

Ultimately, we could consider the following choices to transform $Y$:

* log
* log + standardization
* Box-Cox with $\lambda=-0.25$ (increasing)

##### Feature matrix $\phi$

For the matrix $\phi$, inspired from the statistical approaches in information retrieval, we could consider the following manners of normalization:

* normalization $\ell_2$ along the columns
* term frequencyâ€“inverse document frequency ([tf-idf][1])


## Ridge regression and results
Since the size of kernel matrix is $70975\times 70975$, which is too expensive to inverse, I chose a fast iterative method called FISTA (fast iterative shrinkage-thresholding algorithm), implemented in the toolbox SPAMS by Julien Mairal.

#### - First configuration
* $\phi$: normalization $\ell_2$ along the columns
* $Y$: log + standardization

I put this configuration at the first stage because it shows quite good prediction results.



[1]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf
